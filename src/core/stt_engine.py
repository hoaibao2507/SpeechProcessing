import os
import torch
import librosa
import numpy as np
from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration

class STTEngine:
    def __init__(self, model_path="models/speech_to_text/speech-to-text-vn/whisper-vivos-final"):
        """
        Khá»Ÿi táº¡o model Whisper STT.
        :param model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a model Ä‘Ã£ táº£i vá» tá»« Drive.
        """
        self.model_path = model_path
        self.transcriber = None
        
        if torch.cuda.is_available():
            self.device_str = "cuda" # DÃ¹ng cho model.to()
            self.device_id = 0       # DÃ¹ng cho pipeline()
            print("--- Äang khá»Ÿi táº¡o STT Engine trÃªn GPU (NVIDIA) ---")
        else:
            self.device_str = "cpu"  # DÃ¹ng cho model.to()
            self.device_id = -1      # DÃ¹ng cho pipeline()
            print("--- Äang khá»Ÿi táº¡o STT Engine trÃªn CPU ---")
        self._load_model()

    def _load_model(self):
        try:
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c model táº¡i: {self.model_path}")

            print(f"â³ Äang load model tá»«: {self.model_path}")

            # Load model thá»§ cÃ´ng Ä‘á»ƒ kiá»ƒm soÃ¡t tá»‘t hÆ¡n
            # BÆ°á»›c 1: Load Processor
            self.processor = WhisperProcessor.from_pretrained(self.model_path)
            
            # BÆ°á»›c 2: Load Model vÃ  chuyá»ƒn sang thiáº¿t bá»‹ (Sá»¬A Lá»–I Táº I ÄÃ‚Y)
            # DÃ¹ng self.device_str ("cpu" hoáº·c "cuda") thay vÃ¬ sá»‘ -1
            self.model = WhisperForConditionalGeneration.from_pretrained(self.model_path).to(self.device_str)
            
            # BÆ°á»›c 3: Táº¡o pipeline
            self.transcriber = pipeline(
                "automatic-speech-recognition",
                model=self.model,
                tokenizer=self.processor.tokenizer,
                feature_extractor=self.processor.feature_extractor,
                device=self.device_id # Pipeline thÃ¬ váº«n dÃ¹ng sá»‘ (-1 hoáº·c 0)
            )
            
            print("âœ… Model STT Ä‘Ã£ sáºµn sÃ ng!")
            
        except Exception as e:
            print(f"âŒ Lá»—i load model STT: {e}")
            self.transcriber = None

    def predict(self, audio_path):
        """
        Nháº­n diá»‡n vÄƒn báº£n tá»« file Ã¢m thanh.
        :param audio_path: ÄÆ°á»ng dáº«n file Ã¢m thanh (.wav, .mp3)
        :return: Chuá»—i vÄƒn báº£n káº¿t quáº£
        """
        if self.transcriber is None:
            return "Lá»—i: Model chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh cÃ´ng."

        if not os.path.exists(audio_path):
            return "Lá»—i: KhÃ´ng tÃ¬m tháº¥y file Ã¢m thanh."

        try:
            print(f"ğŸ§ Äang xá»­ lÃ½ file: {audio_path}")
            
            # --- BÆ¯á»šC Xá»¬ LÃ Ã‚M THANH (Giá»‘ng code Colab cá»§a báº¡n) ---
            # Load file vÃ  Ã©p vá» 16kHz (yÃªu cáº§u cá»§a Whisper)
            audio_array, sampling_rate = librosa.load(audio_path, sr=16000)

            # --- BÆ¯á»šC Dá»° ÄOÃN ---
            # generate_kwargs={"language": "vietnamese"} giÃºp AI Ä‘á»‹nh hÆ°á»›ng tá»‘t hÆ¡n
            result = self.transcriber(
                audio_array, 
                generate_kwargs={"language": "vietnamese"}
            )
            
            text = result["text"]
            return text

        except Exception as e:
            print(f"âŒ Lá»—i dá»± Ä‘oÃ¡n: {e}")
            return f"CÃ³ lá»—i xáº£y ra khi xá»­ lÃ½: {str(e)}"

# Äoáº¡n code dÆ°á»›i Ä‘Ã¢y chá»‰ cháº¡y khi báº¡n test file nÃ y trá»±c tiáº¿p
if __name__ == "__main__":
    # Test thá»­
    # Báº¡n nhá»› sá»­a Ä‘Æ°á»ng dáº«n nÃ y trá» Ä‘Ãºng folder model báº¡n táº£i vá»
    engine = STTEngine(model_path="../../models/stt_vn/whisper-vivos-final") 
    
    # Test vá»›i 1 file máº«u
    test_audio = "../../recordings/test.wav"
    if os.path.exists(test_audio):
        print("Káº¿t quáº£:", engine.predict(test_audio))